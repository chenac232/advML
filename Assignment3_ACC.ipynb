{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahikE6LWZD6L"
   },
   "source": [
    "# Text Classification Using the Stanford SST Sentiment Dataset\n",
    "Anne Chen\n",
    "\n",
    "Github: https://github.com/chenac232/advML\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFU8TQ8tZTql"
   },
   "source": [
    "## 1. The dataset\n",
    "Discuss the dataset in general terms and describe why building a predictive model using this data might be practically useful.  Who could benefit from a model like this? Explain.\n",
    "\n",
    "The dataset comprises of positive and negative movie reviews. Building a predictive model using this data might be practically useful as it may help better decipher a true movie review score based on the review rather than the assigned score the reviwer gave, which may be eveluated differently from another reviewer. It may also be helpful in evaluating the movie and determining strong positivity or negativity from reviews in other forums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "booj47wba3SD"
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dp5eCQlEjG40"
   },
   "source": [
    "### AI ModelShare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V4zuRGHbYBUR",
    "outputId": "4cd62f31-b2ab-4e2e-bb8c-8d653422b1ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting aimodelshare==0.0.189\n",
      "  Downloading aimodelshare-0.0.189-py3-none-any.whl (967 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m967.8/967.8 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (2.0.0+cu118)\n",
      "Collecting importlib-resources==5.10.0\n",
      "  Downloading importlib_resources-5.10.0-py3-none-any.whl (34 kB)\n",
      "Collecting skl2onnx>=1.14.0\n",
      "  Downloading skl2onnx-1.14.0-py2.py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.0/294.0 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting onnx==1.12.0\n",
      "  Downloading onnx-1.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydot==1.3.0\n",
      "  Downloading pydot-1.3.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting boto3==1.26.69\n",
      "  Downloading boto3-1.26.69-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting onnxmltools>=1.6.1\n",
      "  Downloading onnxmltools-1.11.2-py2.py3-none-any.whl (322 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keras2onnx>=1.7.0\n",
      "  Downloading keras2onnx-1.7.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.3/96.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy==1.7.0\n",
      "  Downloading scipy-1.7.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.4/28.4 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting PyJWT>=2.4.0\n",
      "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
      "Collecting shortuuid>=1.0.8\n",
      "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Collecting scikit-learn==1.2.1\n",
      "  Downloading scikit_learn-1.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pathlib>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.0.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.6.3)\n",
      "Collecting tf2onnx\n",
      "  Downloading tf2onnx-1.14.0-py3-none-any.whl (451 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.2/451.2 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf==3.19.6\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.9.1 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (5.9.4)\n",
      "Collecting wget==3.2\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting botocore==1.29.82\n",
      "  Downloading botocore-1.29.82-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: seaborn>=0.11.2 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (0.12.2)\n",
      "Collecting Pympler==0.9\n",
      "  Downloading Pympler-0.9.tar.gz (178 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.4/178.4 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting docker==5.0.0\n",
      "  Downloading docker-5.0.0-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.0/147.0 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow==2.9.2\n",
      "  Downloading tensorflow-2.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (2022.10.31)\n",
      "Collecting onnxruntime>=1.7.0\n",
      "  Downloading onnxruntime-1.14.1-cp39-cp39-manylinux_2_27_x86_64.whl (5.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting onnxconverter-common>=1.7.0\n",
      "  Downloading onnxconverter_common-1.13.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse==1.6.3->aimodelshare==0.0.189) (0.40.0)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.9/dist-packages (from astunparse==1.6.3->aimodelshare==0.0.189) (1.16.0)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.9/dist-packages (from botocore==1.29.82->aimodelshare==0.0.189) (1.26.15)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore==1.29.82->aimodelshare==0.0.189) (2.8.2)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.9/dist-packages (from docker==5.0.0->aimodelshare==0.0.189) (1.5.1)\n",
      "Requirement already satisfied: requests!=2.18.0,>=2.14.2 in /usr/local/lib/python3.9/dist-packages (from docker==5.0.0->aimodelshare==0.0.189) (2.27.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources==5.10.0->aimodelshare==0.0.189) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.9/dist-packages (from onnx==1.12.0->aimodelshare==0.0.189) (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from onnx==1.12.0->aimodelshare==0.0.189) (1.22.4)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.9/dist-packages (from pydot==1.3.0->aimodelshare==0.0.189) (3.0.9)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.1->aimodelshare==0.0.189) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.1->aimodelshare==0.0.189) (1.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (0.32.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (67.6.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (0.2.0)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (3.8.0)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.53.0)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (23.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.14.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (0.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (16.0.0)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (2.2.0)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.4.0)\n",
      "Collecting fire\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from onnxruntime>=1.7.0->aimodelshare==0.0.189) (1.11.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.9/dist-packages (from seaborn>=0.11.2->aimodelshare==0.0.189) (3.7.1)\n",
      "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.9/dist-packages (from seaborn>=0.11.2->aimodelshare==0.0.189) (1.5.3)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->aimodelshare==0.0.189) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->aimodelshare==0.0.189) (3.1.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->aimodelshare==0.0.189) (3.11.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->aimodelshare==0.0.189) (3.1)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->aimodelshare==0.0.189) (16.0.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->aimodelshare==0.0.189) (3.25.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (8.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (4.39.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (1.0.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.25->seaborn>=0.11.2->aimodelshare==0.0.189) (2022.7.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare==0.0.189) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare==0.0.189) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare==0.0.189) (2022.12.7)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (2.2.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (2.17.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (1.8.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (3.4.3)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8.1->aimodelshare==0.0.189) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->onnxruntime>=1.7.0->aimodelshare==0.0.189) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (6.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (3.2.2)\n",
      "Building wheels for collected packages: Pympler, wget, fire\n",
      "  Building wheel for Pympler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for Pympler: filename=Pympler-0.9-py3-none-any.whl size=164822 sha256=2c62948717d7520f9a2f90782b39a2efa495f582ce05c8bce6b510f5e7a3342e\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/84/9a/0e7aa69b52bc9dc13ea25679c8d8f1dc11bc5b5289db23d9f3\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9676 sha256=eaaa83bd8abbe3b2adcedb4a0c9b08be8e43c6e260c9ae01c3bd6ddc7ae79985\n",
      "  Stored in directory: /root/.cache/pip/wheels/04/5f/3e/46cc37c5d698415694d83f607f833f83f0149e49b3af9d0f38\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116952 sha256=263026f5099fde31e21cef74d7542a1d7a49a020c5dfdc12c30cfe442edf8fcb\n",
      "  Stored in directory: /root/.cache/pip/wheels/f7/f1/89/b9ea2bf8f80ec027a88fef1d354b3816b4d3d29530988972f6\n",
      "Successfully built Pympler wget fire\n",
      "Installing collected packages: wget, Pympler, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, shortuuid, scipy, PyJWT, pydot, protobuf, keras-preprocessing, jmespath, importlib-resources, humanfriendly, fire, scikit-learn, onnx, docker, coloredlogs, botocore, tf2onnx, s3transfer, onnxruntime, onnxconverter-common, google-auth-oauthlib, tensorboard, skl2onnx, keras2onnx, boto3, tensorflow, onnxmltools, aimodelshare\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.12.0\n",
      "    Uninstalling keras-2.12.0:\n",
      "      Successfully uninstalled keras-2.12.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 23.3.3\n",
      "    Uninstalling flatbuffers-23.3.3:\n",
      "      Successfully uninstalled flatbuffers-23.3.3\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.12.0\n",
      "    Uninstalling tensorflow-estimator-2.12.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.0\n",
      "    Uninstalling tensorboard-data-server-0.7.0:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.1\n",
      "    Uninstalling scipy-1.10.1:\n",
      "      Successfully uninstalled scipy-1.10.1\n",
      "  Attempting uninstall: pydot\n",
      "    Found existing installation: pydot 1.4.2\n",
      "    Uninstalling pydot-1.4.2:\n",
      "      Successfully uninstalled pydot-1.4.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: importlib-resources\n",
      "    Found existing installation: importlib-resources 5.12.0\n",
      "    Uninstalling importlib-resources-5.12.0:\n",
      "      Successfully uninstalled importlib-resources-5.12.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.0.0\n",
      "    Uninstalling google-auth-oauthlib-1.0.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.12.1\n",
      "    Uninstalling tensorboard-2.12.1:\n",
      "      Successfully uninstalled tensorboard-2.12.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.12.0\n",
      "    Uninstalling tensorflow-2.12.0:\n",
      "      Successfully uninstalled tensorflow-2.12.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "arviz 0.15.1 requires scipy>=1.8.0, but you have scipy 1.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyJWT-2.6.0 Pympler-0.9 aimodelshare-0.0.189 boto3-1.26.69 botocore-1.29.82 coloredlogs-15.0.1 docker-5.0.0 fire-0.5.0 flatbuffers-1.12 google-auth-oauthlib-0.4.6 humanfriendly-10.0 importlib-resources-5.10.0 jmespath-1.0.1 keras-2.9.0 keras-preprocessing-1.1.2 keras2onnx-1.7.0 onnx-1.12.0 onnxconverter-common-1.13.0 onnxmltools-1.11.2 onnxruntime-1.14.1 protobuf-3.19.6 pydot-1.3.0 s3transfer-0.6.0 scikit-learn-1.2.1 scipy-1.7.0 shortuuid-1.0.11 skl2onnx-1.14.0 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorflow-2.9.2 tensorflow-estimator-2.9.0 tf2onnx-1.14.0 wget-3.2\n"
     ]
    }
   ],
   "source": [
    "#install aimodelshare library\n",
    "! pip install aimodelshare==0.0.189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDQUMqxXi-vV",
    "outputId": "4c37b9c4-9dca-4483-e67c-9fde8c4db39c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Modelshare Username:··········\n",
      "AI Modelshare Password:··········\n",
      "AI Model Share login credentials set successfully.\n"
     ]
    }
   ],
   "source": [
    "#Set credentials using modelshare.org username/password\n",
    "\n",
    "from aimodelshare.aws import set_credentials\n",
    "    \n",
    "apiurl=\"https://rlxjxnoql9.execute-api.us-east-1.amazonaws.com/prod/m\" #This is the unique rest api that powers this specific Playground\n",
    "\n",
    "set_credentials(apiurl=apiurl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNx60eXLjD-g"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AOBz0SUya5fS",
    "outputId": "3970d29d-ec25-4aa2-8c6a-42a5b4ccb959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading [=============================================>   ]\n",
      "\n",
      "Data downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Get competition data\n",
    "from aimodelshare import download_data\n",
    "download_data('public.ecr.aws/y2e2a1d6/sst2_competition_data-repository:latest') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y6SNp6EMeezL",
    "outputId": "469d2f0f-5974-4c57-9d2e-8de407b33eb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The Rock is destined to be the 21st Century 's...\n",
       "1    The gorgeously elaborate continuation of `` Th...\n",
       "2    Singer/composer Bryan Adams contributes a slew...\n",
       "3                 Yet the act is still charming here .\n",
       "4    Whether or not you 're enlightened by any of D...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up X_train, X_test, and y_train_labels objects\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "X_train=pd.read_csv(\"sst2_competition_data/X_train.csv\", squeeze=True)\n",
    "X_test=pd.read_csv(\"sst2_competition_data/X_test.csv\", squeeze=True)\n",
    "\n",
    "y_train_labels=pd.read_csv(\"sst2_competition_data/y_train_labels.csv\", squeeze=True)\n",
    "\n",
    "# ohe encode Y data\n",
    "y_train = pd.get_dummies(y_train_labels)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DFPorzqZ4mW",
    "outputId": "b1779c59-6aa9-485c-b3fb-9d871a4f8416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6920, 40)\n",
      "(1821, 40)\n"
     ]
    }
   ],
   "source": [
    "# This preprocessor function makes use of the tf.keras tokenizer\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Build vocabulary from training text data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# preprocessor tokenizes words and makes sure all documents have the same length\n",
    "def preprocessor(data, maxlen=40, max_words=10000):\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    X = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "    return X\n",
    "\n",
    "print(preprocessor(X_train).shape)\n",
    "print(preprocessor(X_test).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1IWU-yYaT2T"
   },
   "source": [
    "## 2. Run at least three prediction models to try to predict the SST sentiment dataset well.\n",
    "- Use an Embedding layer and LSTM layers in at least one model\n",
    "- Use an Embedding layer and Conv1d layers in at least one model\n",
    "- Use transfer learning with glove embeddings for at least one of these models\n",
    "\n",
    "Discuss which models performed better and point out relevant hyper-parameter values for successful models.\n",
    "Submit your best three models to the leader board for the SST Model Share competition.\n",
    "\n",
    "Overall, the transfer learning with glove embedding performed best with an accuracy of 0.98, when compared to the models with an embedding layer and an LSTM or 1D convnets layer. However, it may be that this accuracy boost is simply due to the number of epochs specified in the model. I will be running additional models with varying specifications in the next section to confirm this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUKRA7WlZFai"
   },
   "source": [
    "### 2.1 Embedding layer and LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vcr4vUpWa6Fc",
    "outputId": "18a7c9f0-efa3-4185-f7e8-b43b4cd7b181"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_19 (Embedding)    (None, 40, 32)            320000    \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 328,386\n",
      "Trainable params: 328,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "44/44 [==============================] - 5s 59ms/step - loss: 0.6538 - acc: 0.6165 - val_loss: 0.9620 - val_acc: 0.1575\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding,Flatten, SimpleRNN, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32, input_length=40))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model.fit(preprocessor(X_train), y_train,\n",
    "                    epochs=1,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhnTCDhSip9o"
   },
   "source": [
    "#### Model submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFta_pCFippu",
    "outputId": "02205ef7-1abd-44dc-b5ec-ea68082e2893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your preprocessor is now saved to 'preprocessor.zip'\n"
     ]
    }
   ],
   "source": [
    "import aimodelshare as ai\n",
    "ai.export_preprocessor(preprocessor,\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "RVVMbY6Pivc0"
   },
   "outputs": [],
   "source": [
    "# Save keras model to local ONNX file\n",
    "from aimodelshare.aimsonnx import model_to_onnx\n",
    "\n",
    "onnx_model = model_to_onnx(model, framework='keras',\n",
    "                          transfer_learning=False,\n",
    "                          deep_learning=True)\n",
    "\n",
    "with open(\"model.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "cer2f4BnivGw"
   },
   "outputs": [],
   "source": [
    "#Instantiate Competition\n",
    "\n",
    "mycompetition= ai.Competition(apiurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "USFSle_mjUDP",
    "outputId": "1ac4fe4b-8a7d-47ec-e6ae-8510ecd11b17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 9ms/step\n",
      "Insert search tags to help users find your model (optional): group5lstm\n",
      "Provide any useful notes about your model (optional): group5lstm\n",
      "\n",
      "Your model has been submitted as model version 155\n",
      "\n",
      "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
      "\n",
      " https://www.modelshare.org/detail/model:2763\n"
     ]
    }
   ],
   "source": [
    "#Submit Model 1: \n",
    "\n",
    "#-- Generate predicted y values (Model 1)\n",
    "#Note: Keras predict returns the predicted column index location for classification models\n",
    "prediction_column_index=model.predict(preprocessor(X_test)).argmax(axis=1)\n",
    "\n",
    "# extract correct prediction labels \n",
    "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
    "\n",
    "# Submit Model 1 to Competition Leaderboard\n",
    "mycompetition.submit_model(model_filepath = \"model.onnx\",\n",
    "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
    "                                 prediction_submission=prediction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-gzbqOrgYeG"
   },
   "source": [
    "### 2.2 Embedding layer with 1D Convnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOIfJVmtgcIf",
    "outputId": "95b6fb38-ee1a-4a47-85cd-c6e54775ee44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 40, 32)            320000    \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 34, 32)            7200      \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 8, 32)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 2, 32)             7200      \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 32)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 334,466\n",
      "Trainable params: 334,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Use 1D Conv layer rather than RNN or LSTM or GRU to fit model\n",
    "# Why? Much lighter model to fit. Here we are training on the full dataset.  If you try\n",
    "# to build a model using LSTM code after running this one it will be much slower.\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM,Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(10000, 32, input_length=40))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu')) \n",
    "model.add(layers.MaxPooling1D(4)) \n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(2))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zWmQGRd4gfsS",
    "outputId": "2af47ea0-815d-42e4-bb02-15c4fdb43383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/173 [==============================] - 2s 7ms/step - loss: 1.9489 - acc: 0.5226 - val_loss: 0.9785 - val_acc: 0.1488\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(preprocessor(X_train), y_train,\n",
    "                    epochs=1,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeOMg1W_jphn"
   },
   "source": [
    "#### Model submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G2ZHjd-2jroa",
    "outputId": "ea538a95-3a62-479b-8f52-ec9eb3831b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your preprocessor is now saved to 'preprocessor.zip'\n"
     ]
    }
   ],
   "source": [
    "import aimodelshare as ai\n",
    "ai.export_preprocessor(preprocessor,\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yptFi5PwjtAx"
   },
   "outputs": [],
   "source": [
    "# Save keras model to local ONNX file\n",
    "from aimodelshare.aimsonnx import model_to_onnx\n",
    "\n",
    "onnx_model = model_to_onnx(model, framework='keras',\n",
    "                          transfer_learning=False,\n",
    "                          deep_learning=True)\n",
    "\n",
    "with open(\"model2.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3meGRS1CjsyB"
   },
   "outputs": [],
   "source": [
    "#Instantiate Competition\n",
    "\n",
    "mycompetition= ai.Competition(apiurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Js9Iap5gjsju",
    "outputId": "4231bf86-98cd-4432-cfb1-bcaec336cf54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 3ms/step\n",
      "Insert search tags to help users find your model (optional): conv1dgroup5\n",
      "Provide any useful notes about your model (optional): conv1d group 5\n",
      "\n",
      "Your model has been submitted as model version 212\n",
      "\n",
      "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
      "\n",
      " https://www.modelshare.org/detail/model:2763\n"
     ]
    }
   ],
   "source": [
    "#Submit Model 2: \n",
    "\n",
    "#-- Generate predicted y values (Model 2)\n",
    "#Note: Keras predict returns the predicted column index location for classification models\n",
    "prediction_column_index=model.predict(preprocessor(X_test)).argmax(axis=1)\n",
    "\n",
    "# extract correct prediction labels \n",
    "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
    "\n",
    "# Submit Model 1 to Competition Leaderboard\n",
    "mycompetition.submit_model(model_filepath = \"model2.onnx\",\n",
    "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
    "                                 prediction_submission=prediction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnPMCa1ziVSn"
   },
   "source": [
    "### 2.3 Transfer learning with glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cA4yot-YiY2g",
    "outputId": "f7d6e6ac-7474-4069-f174-8e3b5d79e1bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-17 02:55:28--  http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/wordvecs/glove.6B.zip [following]\n",
      "--2023-04-17 02:55:28--  https://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip [following]\n",
      "--2023-04-17 02:55:29--  https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182753 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  4.89MB/s    in 2m 42s  \n",
      "\n",
      "2023-04-17 02:58:11 (5.09 MB/s) - ‘glove.6B.zip’ saved [862182753/862182753]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Download Glove embedding matrix weights \n",
    "! wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-n7K_LaEIdJh",
    "outputId": "a5c26728-f5f1-4b60-9846-11693a47f9d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n",
      "  inflating: glove.6B.50d.txt        \n"
     ]
    }
   ],
   "source": [
    "! unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prNd_cGDIg-T",
    "outputId": "4a5c1d9e-89cf-4f7e-bb74-450462d5e204"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Extract embedding data for 100 feature embedding matrix\n",
    "glove_dir = os.getcwd()\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "PssrriglaD-H"
   },
   "outputs": [],
   "source": [
    "maxlen = 40  # We will cut reviews after 40 words in sequence\n",
    "training_samples = 10000  # We will be training on 10000 samples\n",
    "validation_samples = 10000  # We will be validating on 10000 samples\n",
    "max_words = 10000  # We will only consider the top 10,000 words in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "VL84kDGtaOr5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjH_amV1aL15",
    "outputId": "d273fd01-9ea5-433a-f65b-f420e43fc902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13835 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "X8AzBpdRIlEL"
   },
   "outputs": [],
   "source": [
    "# Build embedding matrix\n",
    "embedding_dim = 100 # change if you use txt files using larger number of features\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1HgfJOVpIqiJ",
    "outputId": "969653f1-1c87-471c-df46-d493aff291ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 40, 100)           1000000   \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 4000)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                128032    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,128,098\n",
      "Trainable params: 1,128,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set up same model architecture as before and then import Glove weights to Embedding layer:\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(10000, embedding_dim, input_length=40))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(2, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FJ5UozzjIsky",
    "outputId": "de51338a-ce38-47d9-d612-bdfc2b8b3f43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "173/173 [==============================] - 2s 7ms/step - loss: 0.6196 - acc: 0.6512 - val_loss: 0.7396 - val_acc: 0.5643\n",
      "Epoch 2/10\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 0.4993 - acc: 0.7543 - val_loss: 0.8180 - val_acc: 0.5470\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 0.4217 - acc: 0.8040 - val_loss: 0.5215 - val_acc: 0.7666\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 0.3542 - acc: 0.8463 - val_loss: 0.7291 - val_acc: 0.6821\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 0.2884 - acc: 0.8802 - val_loss: 1.0665 - val_acc: 0.5571\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 0.2268 - acc: 0.9187 - val_loss: 0.9810 - val_acc: 0.6199\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 0.1739 - acc: 0.9417 - val_loss: 0.9506 - val_acc: 0.6402\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 0.1332 - acc: 0.9588 - val_loss: 0.9365 - val_acc: 0.6842\n",
      "Epoch 9/10\n",
      "173/173 [==============================] - 1s 4ms/step - loss: 0.0979 - acc: 0.9765 - val_loss: 1.0315 - val_acc: 0.6604\n",
      "Epoch 10/10\n",
      "173/173 [==============================] - 1s 4ms/step - loss: 0.0719 - acc: 0.9834 - val_loss: 0.9229 - val_acc: 0.7363\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add weights in same manner as transfer learning and turn of trainable option before fitting model to freeze weights.\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(preprocessor(X_train), y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)\n",
    "model.save_weights('pre_trained_glove_model.h5')\n",
    "\n",
    "\n",
    "# Training data small to speed up training. Increase for better fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpU6hApkkGQX"
   },
   "source": [
    "#### Model submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KjTzRlJvkH8p",
    "outputId": "944cef3a-b561-4259-aede-88e1c26c9b23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your preprocessor is now saved to 'preprocessor.zip'\n"
     ]
    }
   ],
   "source": [
    "import aimodelshare as ai\n",
    "ai.export_preprocessor(preprocessor,\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "B3xe6V_FkITz"
   },
   "outputs": [],
   "source": [
    "# Save keras model to local ONNX file\n",
    "from aimodelshare.aimsonnx import model_to_onnx\n",
    "\n",
    "onnx_model = model_to_onnx(model, framework='keras',\n",
    "                          transfer_learning=False,\n",
    "                          deep_learning=True)\n",
    "\n",
    "with open(\"model3.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "RtYmdOMAkIxK"
   },
   "outputs": [],
   "source": [
    "#Instantiate Competition\n",
    "\n",
    "mycompetition= ai.Competition(apiurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55MAFULgkJKe",
    "outputId": "0ce1050f-1939-4a95-b57e-750d74029f6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 2ms/step\n",
      "Insert search tags to help users find your model (optional): glove1\n",
      "Provide any useful notes about your model (optional): glove1\n",
      "\n",
      "Your model has been submitted as model version 265\n",
      "\n",
      "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
      "\n",
      " https://www.modelshare.org/detail/model:2763\n"
     ]
    }
   ],
   "source": [
    "#Submit Model 3: \n",
    "\n",
    "#-- Generate predicted y values (Model 2)\n",
    "#Note: Keras predict returns the predicted column index location for classification models\n",
    "prediction_column_index=model.predict(preprocessor(X_test)).argmax(axis=1)\n",
    "\n",
    "# extract correct prediction labels \n",
    "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
    "\n",
    "# Submit Model 1 to Competition Leaderboard\n",
    "mycompetition.submit_model(model_filepath = \"model3.onnx\",\n",
    "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
    "                                 prediction_submission=prediction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqM-dsXFaVKk"
   },
   "source": [
    "## 3. Three more models after team discussion\n",
    "After you submit your first three models, describe your best model with your team via your team slack channel\n",
    "Fit and submit up to three more models after learning from your team.\n",
    "Discuss results\n",
    "\n",
    "**Team Discussion:** In the team discussion, we found that increasing the number of epochs for each model specification yielded to the greatest boost in accuracy score.\n",
    "\n",
    "**Updated model results:**\n",
    "- For the model with the LSTM layer, when I increase the epoch = 10, the accuracy score improves to nearly 0.8 (from a score of 0.5 with epoch=1).\n",
    "- Increasing epochs for the 1D convnets model did not yield a similar boost in accuracy score, however, it stayed around 0.5-0.6.\n",
    "- For the glove embeddings model, instead of increasing epochs I increased the number of units in the dense layer from 32 to 64. This yielded minimal improvements from the initial model in part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_ixVatfkg3p"
   },
   "source": [
    "### Model 1 - LSTM + embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TJFLtr449TEZ",
    "outputId": "a71b3e21-8ff1-4359-f071-cd3b6e1ba1d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 40, 32)            320000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 328,386\n",
      "Trainable params: 328,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "44/44 [==============================] - 4s 43ms/step - loss: 0.6490 - acc: 0.6172 - val_loss: 0.7548 - val_acc: 0.4328\n",
      "Epoch 2/10\n",
      "44/44 [==============================] - 1s 31ms/step - loss: 0.5140 - acc: 0.7558 - val_loss: 0.6850 - val_acc: 0.5860\n",
      "Epoch 3/10\n",
      "44/44 [==============================] - 1s 31ms/step - loss: 0.3904 - acc: 0.8441 - val_loss: 0.6837 - val_acc: 0.7081\n",
      "Epoch 4/10\n",
      "44/44 [==============================] - 1s 30ms/step - loss: 0.2905 - acc: 0.8873 - val_loss: 1.2055 - val_acc: 0.5036\n",
      "Epoch 5/10\n",
      "44/44 [==============================] - 1s 31ms/step - loss: 0.2203 - acc: 0.9155 - val_loss: 0.8539 - val_acc: 0.6561\n",
      "Epoch 6/10\n",
      "44/44 [==============================] - 2s 41ms/step - loss: 0.1705 - acc: 0.9362 - val_loss: 0.6557 - val_acc: 0.7211\n",
      "Epoch 7/10\n",
      "44/44 [==============================] - 2s 46ms/step - loss: 0.1315 - acc: 0.9523 - val_loss: 0.4440 - val_acc: 0.8280\n",
      "Epoch 8/10\n",
      "44/44 [==============================] - 2s 47ms/step - loss: 0.1058 - acc: 0.9615 - val_loss: 0.6601 - val_acc: 0.7637\n",
      "Epoch 9/10\n",
      "44/44 [==============================] - 2s 46ms/step - loss: 0.0825 - acc: 0.9724 - val_loss: 0.8935 - val_acc: 0.7298\n",
      "Epoch 10/10\n",
      "44/44 [==============================] - 2s 41ms/step - loss: 0.0672 - acc: 0.9772 - val_loss: 0.7191 - val_acc: 0.7868\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding,Flatten, SimpleRNN, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32, input_length=40))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model.fit(preprocessor(X_train), y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGrsdnQ1kWyH"
   },
   "source": [
    "#### Model submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6S1Eiq-Bkzr7",
    "outputId": "3a61fdbd-5d7a-4242-9f85-4d27bebc0083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your preprocessor is now saved to 'preprocessor.zip'\n"
     ]
    }
   ],
   "source": [
    "import aimodelshare as ai\n",
    "ai.export_preprocessor(preprocessor,\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1zPaVhRmkzPM"
   },
   "outputs": [],
   "source": [
    "# Save keras model to local ONNX file\n",
    "from aimodelshare.aimsonnx import model_to_onnx\n",
    "\n",
    "onnx_model = model_to_onnx(model, framework='keras',\n",
    "                          transfer_learning=False,\n",
    "                          deep_learning=True)\n",
    "\n",
    "with open(\"model4.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kFq_kLa-kzG3"
   },
   "outputs": [],
   "source": [
    "#Instantiate Competition\n",
    "\n",
    "mycompetition= ai.Competition(apiurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLxQU0E4ky-D",
    "outputId": "d55f0738-926e-408e-b017-b4353b97118b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 7ms/step\n",
      "Insert search tags to help users find your model (optional): lstm model 2\n",
      "Provide any useful notes about your model (optional): lstm model 2\n",
      "\n",
      "Your model has been submitted as model version 218\n",
      "\n",
      "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
      "\n",
      " https://www.modelshare.org/detail/model:2763\n"
     ]
    }
   ],
   "source": [
    "#Submit Model 3: \n",
    "\n",
    "#-- Generate predicted y values (Model 2)\n",
    "#Note: Keras predict returns the predicted column index location for classification models\n",
    "prediction_column_index=model.predict(preprocessor(X_test)).argmax(axis=1)\n",
    "\n",
    "# extract correct prediction labels \n",
    "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
    "\n",
    "# Submit Model 1 to Competition Leaderboard\n",
    "mycompetition.submit_model(model_filepath = \"model4.onnx\",\n",
    "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
    "                                 prediction_submission=prediction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxQhPwrOkjDF"
   },
   "source": [
    "### Model 2 - 1D conv + embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rb6Jhtou_ILn",
    "outputId": "5005f452-66b5-4dd9-bd20-b88d4e37497f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    (None, 40, 32)            320000    \n",
      "                                                                 \n",
      " conv1d_17 (Conv1D)          (None, 34, 32)            7200      \n",
      "                                                                 \n",
      " max_pooling1d_12 (MaxPoolin  (None, 8, 32)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_18 (Conv1D)          (None, 2, 64)             14400     \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Glo  (None, 64)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 341,730\n",
      "Trainable params: 341,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Use 1D Conv layer rather than RNN or LSTM or GRU to fit model\n",
    "# Why? Much lighter model to fit. Here we are training on the full dataset.  If you try\n",
    "# to build a model using LSTM code after running this one it will be much slower.\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM,Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(10000, 32, input_length=40))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu')) \n",
    "model.add(layers.MaxPooling1D(4)) \n",
    "model.add(layers.Conv1D(64, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(2))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFqmtJHi_I1R",
    "outputId": "4d8bf06d-fbd2-4939-f764-82b0052cc507"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "173/173 [==============================] - 3s 10ms/step - loss: 0.9629 - acc: 0.6142 - val_loss: 0.8863 - val_acc: 0.1488\n",
      "Epoch 2/10\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 0.6653 - acc: 0.6149 - val_loss: 0.8720 - val_acc: 0.1488\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 0.6613 - acc: 0.6149 - val_loss: 0.8890 - val_acc: 0.1488\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 0.6567 - acc: 0.6149 - val_loss: 0.9329 - val_acc: 0.1488\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 0.6511 - acc: 0.6149 - val_loss: 0.8416 - val_acc: 0.1488\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 0.6432 - acc: 0.6149 - val_loss: 0.8859 - val_acc: 0.1488\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 0.6312 - acc: 0.6149 - val_loss: 0.8941 - val_acc: 0.1488\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 0.6121 - acc: 0.6248 - val_loss: 0.8808 - val_acc: 0.1597\n",
      "Epoch 9/10\n",
      "173/173 [==============================] - 1s 6ms/step - loss: 0.5845 - acc: 0.6492 - val_loss: 0.8348 - val_acc: 0.2486\n",
      "Epoch 10/10\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 0.5458 - acc: 0.7061 - val_loss: 0.8854 - val_acc: 0.3035\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(preprocessor(X_train), y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w45rOL62km-4"
   },
   "source": [
    "#### Model submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "JQp5Oo5kAbEx"
   },
   "outputs": [],
   "source": [
    "import aimodelshare as ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "aL0glOgMAeI2"
   },
   "outputs": [],
   "source": [
    "# Save keras model to local ONNX file\n",
    "from aimodelshare.aimsonnx import model_to_onnx\n",
    "\n",
    "onnx_model = model_to_onnx(model, framework='keras',\n",
    "                          transfer_learning=False,\n",
    "                          deep_learning=True)\n",
    "\n",
    "with open(\"model5.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "JS_qArl3AgSw"
   },
   "outputs": [],
   "source": [
    "#Instantiate Competition\n",
    "\n",
    "mycompetition= ai.Competition(apiurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPmGEHB8AiSM",
    "outputId": "0bfed5c4-902b-41ca-b89d-0f751b2d696b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 3ms/step\n",
      "Insert search tags to help users find your model (optional): convnets\n",
      "Provide any useful notes about your model (optional): convnets\n",
      "\n",
      "Your model has been submitted as model version 251\n",
      "\n",
      "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
      "\n",
      " https://www.modelshare.org/detail/model:2763\n"
     ]
    }
   ],
   "source": [
    "#Submit Model 3: \n",
    "\n",
    "#-- Generate predicted y values (Model 2)\n",
    "#Note: Keras predict returns the predicted column index location for classification models\n",
    "prediction_column_index=model.predict(preprocessor(X_test)).argmax(axis=1)\n",
    "\n",
    "# extract correct prediction labels \n",
    "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
    "\n",
    "# Submit Model 1 to Competition Leaderboard\n",
    "mycompetition.submit_model(model_filepath = \"model5.onnx\",\n",
    "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
    "                                 prediction_submission=prediction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1nAf1nCkqXv"
   },
   "source": [
    "### Model 3 - transfer learning with glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "DcTOzGhIKO-G"
   },
   "outputs": [],
   "source": [
    "# Build embedding matrix\n",
    "embedding_dim = 100 # change if you use txt files using larger number of features\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qLK6MFwkGguo",
    "outputId": "54ed0019-2e24-4267-b80c-40bfce904135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 40, 100)           1000000   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4000)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                256064    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,256,194\n",
      "Trainable params: 1,256,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set up same model architecture as before and then import Glove weights to Embedding layer:\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(10000, embedding_dim, input_length=40))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(2, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yS4mfi41v-wN",
    "outputId": "7475fb74-48fa-4935-f295-78ae7f6e82bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "173/173 [==============================] - 2s 6ms/step - loss: 0.6272 - acc: 0.6393 - val_loss: 0.6874 - val_acc: 0.6257\n",
      "Epoch 2/10\n",
      "173/173 [==============================] - 1s 5ms/step - loss: 0.4934 - acc: 0.7561 - val_loss: 0.6634 - val_acc: 0.6763\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 1s 5ms/step - loss: 0.4019 - acc: 0.8177 - val_loss: 0.8755 - val_acc: 0.5882\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 1s 5ms/step - loss: 0.3143 - acc: 0.8748 - val_loss: 0.7217 - val_acc: 0.6821\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 0.2299 - acc: 0.9133 - val_loss: 0.9233 - val_acc: 0.6416\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 1s 7ms/step - loss: 0.1591 - acc: 0.9532 - val_loss: 0.8680 - val_acc: 0.6785\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 0.1076 - acc: 0.9702 - val_loss: 0.9019 - val_acc: 0.6944\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 0.0701 - acc: 0.9834 - val_loss: 1.3655 - val_acc: 0.6048\n",
      "Epoch 9/10\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 0.0421 - acc: 0.9926 - val_loss: 1.5685 - val_acc: 0.5975\n",
      "Epoch 10/10\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 0.0257 - acc: 0.9966 - val_loss: 1.1016 - val_acc: 0.7074\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add weights in same manner as transfer learning and turn of trainable option before fitting model to freeze weights.\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(preprocessor(X_train), y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)\n",
    "model.save_weights('pre_trained_glove_model.h5')\n",
    "\n",
    "\n",
    "# Training data small to speed up training. Increase for better fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFnoNGAvku2m"
   },
   "source": [
    "#### Model submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2iUwHCN9ktuN",
    "outputId": "20a72e59-69fd-49fe-c6d0-c31ccc8249ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your preprocessor is now saved to 'preprocessor.zip'\n"
     ]
    }
   ],
   "source": [
    "import aimodelshare as ai\n",
    "ai.export_preprocessor(preprocessor,\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "dLZqX6Qrv6Qx"
   },
   "outputs": [],
   "source": [
    "# Save keras model to local ONNX file\n",
    "from aimodelshare.aimsonnx import model_to_onnx\n",
    "\n",
    "onnx_model = model_to_onnx(model, framework='keras',\n",
    "                          transfer_learning=False,\n",
    "                          deep_learning=True)\n",
    "\n",
    "with open(\"model5.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "e40kk7nSv72j"
   },
   "outputs": [],
   "source": [
    "#Instantiate Competition\n",
    "\n",
    "mycompetition= ai.Competition(apiurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UlRAu770v9ow",
    "outputId": "88546a9d-02b1-4979-b752-97557aa95859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 2ms/step\n",
      "Insert search tags to help users find your model (optional): glove embedding model 2\n",
      "Provide any useful notes about your model (optional): glove embedding model 2\n",
      "\n",
      "Your model has been submitted as model version 264\n",
      "\n",
      "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
      "\n",
      " https://www.modelshare.org/detail/model:2763\n"
     ]
    }
   ],
   "source": [
    "#Submit Model 3: \n",
    "\n",
    "#-- Generate predicted y values (Model 2)\n",
    "#Note: Keras predict returns the predicted column index location for classification models\n",
    "prediction_column_index=model.predict(preprocessor(X_test)).argmax(axis=1)\n",
    "\n",
    "# extract correct prediction labels \n",
    "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
    "\n",
    "# Submit Model 1 to Competition Leaderboard\n",
    "mycompetition.submit_model(model_filepath = \"model5.onnx\",\n",
    "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
    "                                 prediction_submission=prediction_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-4BXNf6atBN"
   },
   "source": [
    "## 4. Which model performed best? \n",
    "Discuss which models you tried and which models performed better and point out relevant hyper-parameter values for successful models.\n",
    "\n",
    "Overall, the LSTM and embeddings layer with epochs =10 yielded the greatest amount of model accuracy, while the convnets model had the poorest initial accuracy. Increasing epochs in the convnets model did not boost model accuracy to the same magnitude when the LSTM model was boosted. The glove embeddings model performed well initially with epochs=10. Adding the number of units in the dense layer from 32 to 64 did not amount to much of a model accuracy boost. Ultimately, the number of epochs was a great determinant in improving model accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KolRxpC34V5j"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
